{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTE (Recognizing Textual Entailment) with transformers\n",
    "## Using a pretrained transformer fine-tuned on MNLI for fine-tuning on SNLI\n",
    "Inspired by Keras code example [Semantic Similarity with BERT](https://keras.io/examples/nlp/semantic_similarity_with_bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, ModelCheckpoint, EarlyStopping\n",
    "from transformers import AutoTokenizer, AdamW, get_constant_schedule_with_warmup, DebertaModel\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128*2\n",
    "HUB_MODEL_CHECKPOINT = 'microsoft/deberta-base-mnli'\n",
    "MODEL_NAME = HUB_MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "PROJECT_NAME = f'{MODEL_NAME}-finetuned-snli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99964db69743e8b12755bbd6ad32a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-21d54e6470652178.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-b746e1998966e2f4.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-89fb34b79586ce05.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'labels'],\n",
       "        num_rows: 9824\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'labels'],\n",
       "        num_rows: 549367\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'labels'],\n",
       "        num_rows: 9842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset('snli')\n",
    "dataset = dataset.filter(lambda example: example['label'] != -1) \n",
    "dataset = dataset.rename_column('label', 'labels')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 250, 621, 15, 10, 5253, 13855, 81, 10, 3187, 159, 16847, 4, 2, 250, 621, 16, 1058, 39, 5253, 13, 10, 1465, 4, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUB_MODEL_CHECKPOINT)\n",
    "\n",
    "example = dataset['train'][0]\n",
    "tokenizer(example['premise'], example['hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-89e08e2169e07c1c.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-5fc1ed23f08797ae.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-4ac3457c935c3968.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example['premise'], \n",
    "                     example['hypothesis'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH, \n",
    "                     truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "for key in dataset.keys():\n",
    "    dataset[key].set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([    1,   250,   621,    15,    10,  5253, 13855,    81,    10,  3187,\n",
       "           159, 16847,     4,     2,   250,   621,    16,  1058,    39,  5253,\n",
       "            13,    10,  1465,     4,     2,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOCAL_MODEL_CHECKPOINT = f'./{PROJECT_NAME}/checkpoint-189'\n",
    "\n",
    "# bert = DebertaModel.from_pretrained(HUB_MODEL_CHECKPOINT)\n",
    "# bert_output = bert(\n",
    "#     input_ids=example['input_ids'].unsqueeze(0),\n",
    "#     attention_mask=example['attention_mask'].unsqueeze(0),\n",
    "#     token_type_ids=example['token_type_ids'].unsqueeze(0)\n",
    "#     )\n",
    "# bert_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _loader = DataLoader(dataset['train'], batch_size=3, shuffle=False)\n",
    "# _batch = next(iter(_loader))\n",
    "# _batch.pop('labels')\n",
    "# # _sequence_embeddings = bert(**_batch).pooler_output\n",
    "# _sequence_embeddings = bert(**_batch).last_hidden_state[..., 0, :]\n",
    "# print(_sequence_embeddings.shape)\n",
    "# _clf = torch.nn.Linear(768, 3)\n",
    "# _clf(_sequence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNLIModel(LightningModule):\n",
    "        \n",
    "    def __init__(self, \n",
    "                 model_checkpoint,\n",
    "                 num_labels=3,\n",
    "                 freeze_bert=True,\n",
    "                 learning_rate=1e-3,\n",
    "                 warmup_steps=10,\n",
    "                 weight_decay=0.01\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = DebertaModel.from_pretrained(model_checkpoint)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.functional.accuracy\n",
    "        \n",
    "    def forward(self, features):\n",
    "        x = self.bert(**features).last_hidden_state[..., 0, :]\n",
    "        # Could include Linear + Tanh after as in BERT\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def _get_preds_loss_accuracy(self, batch):\n",
    "        y = batch.pop('labels')\n",
    "        y_hat = self(batch)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        return preds, loss, acc, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss, acc, _ = self._get_preds_loss_accuracy(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_accuracy', acc)\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        preds, loss, acc, labels = self._get_preds_loss_accuracy(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', acc)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"labels\": labels, 'accuracy': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, loss, acc, _ = self._get_preds_loss_accuracy(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "        return {\"loss\": loss, 'accuracy': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        acc = self.accuracy(preds, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", acc, prog_bar=True)\n",
    "        self.log_dict({'accuracy': acc}, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.classifier.parameters(), \n",
    "                          lr=self.hparams.learning_rate, \n",
    "                          weight_decay=self.hparams.weight_decay, \n",
    "                          correct_bias=False)\n",
    "        scheduler = get_constant_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaModel: ['pooler.dense.weight', 'classifier.weight', 'config', 'pooler.dense.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SAMPLES = 10000\n",
    "EVAL_SAMPLES = 1000\n",
    "BATCH_SIZE = 32\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
    "MAX_EPOCHS = 3\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "FREEZE_BERT = True\n",
    "\n",
    "wandb_logger = WandbLogger(project=PROJECT_NAME)\n",
    "\n",
    "train_ds = dataset['train'].select(np.random.randint(0, dataset['train'].num_rows, TRAIN_SAMPLES).tolist())\n",
    "eval_ds = dataset['validation'].select(np.random.randint(0, dataset['validation'].num_rows, EVAL_SAMPLES).tolist())\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE, num_workers=6)\n",
    "eval_dataloader = DataLoader(eval_ds, shuffle=False, batch_size=BATCH_SIZE, num_workers=6)\n",
    "\n",
    "model = BertNLIModel(HUB_MODEL_CHECKPOINT, \n",
    "                     freeze_bert=FREEZE_BERT,\n",
    "                     learning_rate=LR,\n",
    "                     weight_decay=WEIGHT_DECAY,\n",
    "                    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    default_root_dir=PROJECT_NAME,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        TQDMProgressBar(refresh_rate=1), \n",
    "        ModelCheckpoint(monitor='val_accuracy', mode='max'),\n",
    "        EarlyStopping('val_loss', patience=1, min_delta=0.1, mode='min')\n",
    "        ],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    precision=16,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs   \n",
    "    accumulate_grad_batches=PER_DEVICE_TRAIN_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pytorch_lightning/loggers/wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | bert       | DebertaModel     | 138 M \n",
      "1 | classifier | Linear           | 2.3 K \n",
      "2 | loss       | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "138 M     Non-trainable params\n",
      "138 M     Total params\n",
      "277.208   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1881fe8461b54ca4b941c6eeb404896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6729a59f134b3598a70a895d8953d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d7249916434e66b8e971b911314195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f227caa8e6784e7d80beaa5c10bced38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py:1399: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  f\"`.{fn}(ckpt_path=None)` was called without a model.\"\n",
      "Restoring states from the checkpoint path at deberta-base-mnli-finetuned-snli/deberta-base-mnli-finetuned-snli/q4tyf11v/checkpoints/epoch=1-step=313.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at deberta-base-mnli-finetuned-snli/deberta-base-mnli-finetuned-snli/q4tyf11v/checkpoints/epoch=1-step=313.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4dc867a9e54c819ec4d5dd33258ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.8686889410018921, 'test_loss': 0.38150331377983093}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.38150331377983093, 'test_accuracy': 0.8686889410018921}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = dataset['test']\n",
    "test_dataloader = DataLoader(test_ds, shuffle=False, batch_size=BATCH_SIZE, num_workers=6)\n",
    "\n",
    "trainer.test(dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a73908284cc4a9c81f3a73d28cdfb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▅▅▅▅█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▆▇▆▁▄▃█▃</td></tr><tr><td>train_loss</td><td>▄▁▂▅█▅▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▂▁▂▄▄▅▆███</td></tr><tr><td>val_accuracy</td><td>▁█</td></tr><tr><td>val_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.869</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.86869</td></tr><tr><td>test_loss</td><td>0.3815</td></tr><tr><td>train_accuracy</td><td>0.75</td></tr><tr><td>train_loss</td><td>0.54398</td></tr><tr><td>trainer/global_step</td><td>314</td></tr><tr><td>val_accuracy</td><td>0.869</td></tr><tr><td>val_loss</td><td>0.37473</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zany-spaceship-10</strong>: <a href=\"https://wandb.ai/thierry-wendling-research/deberta-base-mnli-finetuned-snli/runs/q4tyf11v\" target=\"_blank\">https://wandb.ai/thierry-wendling-research/deberta-base-mnli-finetuned-snli/runs/q4tyf11v</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220929_025456-q4tyf11v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5517fee858da220e354b0e7f8c879a17a674f17dc1b2bfda8d8bbe6e302f27df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
