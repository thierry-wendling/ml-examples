{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTE (Recognizing Textual Entailment) with DeBERTa\n",
    "## Using a pretrained DeBERTa model fine-tuned on MNLI for zero-shot text classification on SNLI\n",
    "Inspired by Keras code example [Semantic Similarity with BERT](https://keras.io/examples/nlp/semantic_similarity_with_bert/)\n",
    "\n",
    "Executed on AWS SageMaker `ml.g4dn.2xlarge` GPU instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch] accelerate datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, \n",
    "    AdamW, get_scheduler\n",
    "#     EarlyStoppingCallback,\n",
    "#     Trainer, TrainingArguments\n",
    "    )\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 3\n",
    "MAX_LENGTH = 128\n",
    "HUB_MODEL_CHECKPOINT = 'bert-base-uncased'\n",
    "MODEL_NAME = HUB_MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "# LOCAL_MODEL_CHECKPOINT = f'./{MODEL_NAME}-finetuned-snli/checkpoint-XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86effa8909f34fa2ada26abf1b843dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-21d54e6470652178.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-b746e1998966e2f4.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-89fb34b79586ce05.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'labels'],\n",
       "        num_rows: 9824\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'labels'],\n",
       "        num_rows: 549367\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'labels'],\n",
       "        num_rows: 9842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('snli')\n",
    "dataset = dataset.filter(lambda example: example['label'] != -1) \n",
    "dataset = dataset.rename_column('label', 'labels')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUB_MODEL_CHECKPOINT)\n",
    "\n",
    "example = dataset['train'][0]\n",
    "tokenizer(example['premise'], example['hypothesis'], return_token_type_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-3e1d4c5a625bb5e4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b937f8f7ce4d9eaa7eee058582ccd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-96104474684b9b7a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example['premise'], \n",
    "                     example['hypothesis'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH, \n",
    "                     return_token_type_ids=True,\n",
    "                     return_attention_mask=True,\n",
    "                     truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "for key in dataset.keys():\n",
    "    dataset[key].set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 2]),\n",
       " 'input_ids': tensor([[  101,  1037,  2711,  2006,  1037,  3586, 14523,  2058,  1037,  3714,\n",
       "           2091, 13297,  1012,   102,  1037,  2711,  2003,  2731,  2010,  3586,\n",
       "           2005,  1037,  2971,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1037,  2711,  2006,  1037,  3586, 14523,  2058,  1037,  3714,\n",
       "           2091, 13297,  1012,   102,  1037,  2711,  2003,  2012,  1037, 15736,\n",
       "           1010, 13063,  2019, 18168, 12260,  4674,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = dataset['train'][0:2]\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_trainable_params(model):\n",
    "    return np.sum(np.array([p.numel() for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model_checkpoint, num_labels=3):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "#         self.apply(self._init_weights)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        features = {k: v for k, v in features.items() if k in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
    "        embeddings = self.bert(**features).pooler_output ### CLS pooling\n",
    "        return self.classifier(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5563,  0.0554, -0.6191],\n",
       "        [ 0.5499,  0.0641, -0.6162]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTClassifier(model_checkpoint=HUB_MODEL_CHECKPOINT)\n",
    "model(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of trainable params: 109M\n",
      "Actual number of trainable params: 109484547\n"
     ]
    }
   ],
   "source": [
    "FREEZE_ENCODER = False\n",
    "\n",
    "\n",
    "assert model.num_labels == NUM_LABELS, f'The number of labels should be {NUM_LABELS}'\n",
    "print(f'Original number of trainable params: {round(get_number_of_trainable_params(model)/1_000_000)}M')\n",
    "\n",
    "if FREEZE_ENCODER:\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith('classifier'):\n",
    "            param.requires_grad = False\n",
    "\n",
    "print(f'Actual number of trainable params: {get_number_of_trainable_params(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, eps, warmup_steps, weight_decay, num_training_steps):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=eps, correct_bias=False, weight_decay=weight_decay)\n",
    "    scheduler = get_scheduler('linear', optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    correct = (predictions == labels).float()\n",
    "    accuracy = correct.mean()\n",
    "    return {'accuracy': accuracy, 'correct': correct}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(accelerator, model, dataloader, optimizer, scheduler, loss_fn):\n",
    "    epoch_size = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader):        \n",
    "        optimizer.zero_grad() # clear gradients first\n",
    "        labels = batch['labels']\n",
    "        batch_size = labels.shape[0]\n",
    "        predictions = model(batch)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            metrics = compute_metrics(accelerator.gather(predictions), accelerator.gather(labels))\n",
    "            correct = metrics['correct'].cpu().numpy().sum()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_correct += correct\n",
    "            epoch_size += batch_size\n",
    "            \n",
    "    return {'train_loss': round(epoch_loss/epoch_size, 5), 'train_accuracy': epoch_correct/epoch_size, 'lr': scheduler.get_last_lr()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(accelerator, model, dataloader, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_size = 0\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader):   \n",
    "        labels = batch['labels']\n",
    "        batch_size = labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        metrics = compute_metrics(accelerator.gather(predictions), accelerator.gather(labels))\n",
    "        correct = metrics['correct'].cpu().numpy().sum()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        epoch_size += batch_size\n",
    "    return {'eval_loss': round(epoch_loss/epoch_size, 5), 'eval_accuracy': epoch_correct/epoch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective training batch size: 32\n",
      "Number of training steps per epoch: 313\n",
      "Max number of epochs: 3\n",
      "Total training steps: 939\n",
      "Warmup steps: 187\n"
     ]
    }
   ],
   "source": [
    "FREEZE_ENCODER = True\n",
    "TRAIN_SAMPLES = 10000\n",
    "EVAL_SAMPLES = 1000\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "TRAIN_BATCH_SIZE = GRADIENT_ACCUMULATION_STEPS * PER_DEVICE_TRAIN_BATCH_SIZE\n",
    "print(f'Effective training batch size: {TRAIN_BATCH_SIZE}')\n",
    "EVAL_BATCH_SIZE = 100\n",
    "TRAIN_STEPS_PER_EPOCH = math.ceil(TRAIN_SAMPLES/TRAIN_BATCH_SIZE)\n",
    "print(f'Number of training steps per epoch: {TRAIN_STEPS_PER_EPOCH}')\n",
    "MAX_EPOCHS = 3\n",
    "print(f'Max number of epochs: {MAX_EPOCHS}')\n",
    "LR = 2e-5\n",
    "EPS = 1e-6\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_PERCENT = 0.2\n",
    "TOTAL_STEPS = MAX_EPOCHS * TRAIN_STEPS_PER_EPOCH\n",
    "print(f'Total training steps: {TOTAL_STEPS}')\n",
    "WARMUP_STEPS = int(TOTAL_STEPS*WARMUP_PERCENT)\n",
    "print(f'Warmup steps: {WARMUP_STEPS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(seed=42, mixed_precision='fp16'):\n",
    "    set_seed(seed)\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision)\n",
    "\n",
    "    train_ds = dataset['train'].shuffle(seed=SEED).select(range(TRAIN_SAMPLES))\n",
    "    eval_ds = dataset['validation'].shuffle(seed=SEED).select(range(EVAL_SAMPLES))\n",
    "    train_dataloader = DataLoader(train_ds, num_workers=os.cpu_count(), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    eval_dataloader = DataLoader(eval_ds, num_workers=os.cpu_count(), batch_size=EVAL_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = BERTClassifier(model_checkpoint=HUB_MODEL_CHECKPOINT)\n",
    "    if FREEZE_ENCODER:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not name.startswith('classifier'):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    optimizer, lr_scheduler = configure_optimizer(model, LR, EPS, WARMUP_STEPS, WEIGHT_DECAY, TOTAL_STEPS)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler, eval_dataloader\n",
    "    )\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        train_metrics = train(accelerator, model, train_dataloader, optimizer, lr_scheduler, loss_fn)\n",
    "        eval_metrics = evaluate(accelerator, model, eval_dataloader, loss_fn)\n",
    "        epoch_metrics = {**train_metrics, **eval_metrics}\n",
    "        accelerator.print(f\"epoch {epoch}: {epoch_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-c87be39ba90012f8.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ec2-user/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-9ba45445327e9c76.arrow\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "100%|██████████| 313/313 [01:15<00:00,  4.15it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'train_loss': 0.02406, 'train_accuracy': 0.6576, 'lr': [1.670212765957447e-05], 'eval_loss': 0.00537, 'eval_accuracy': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:17<00:00,  4.02it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'train_loss': 0.01304, 'train_accuracy': 0.8472, 'lr': [8.377659574468086e-06], 'eval_loss': 0.00516, 'eval_accuracy': 0.812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:16<00:00,  4.10it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'train_loss': 0.00696, 'train_accuracy': 0.9267, 'lr': [5.319148936170213e-08], 'eval_loss': 0.0059, 'eval_accuracy': 0.806}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_NAME = f'{MODEL_NAME}-finetuned-snli'\n",
    "\n",
    "# wandb.init(project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5517fee858da220e354b0e7f8c879a17a674f17dc1b2bfda8d8bbe6e302f27df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
